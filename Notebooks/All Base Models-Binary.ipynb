{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "import os \n",
    "import time \n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "# #ISIC 2019 25000 IMAGES DATASET LOCATION\n",
    "# train_path = \"/home/mrb685/Datasets/Thesis Final Dataset/train/\"\n",
    "# val_path  = \"/home/mrb685/Datasets/Thesis Final Dataset/val/\"\n",
    "# test_path = \"/home/mrb685/Datasets/Thesis Final Dataset/test/\"\n",
    "\n",
    "# # U-net cropped dataset \n",
    "# train_path = \"/home/mrb685/Datasets/BCDU-Segmentations-Cropped/train/\"\n",
    "# val_path  = \"/home/mrb685/Datasets/BCDU-Segmentations-Cropped/val/\"\n",
    "# test_path = \"/home/mrb685/Datasets/BCDU-Segmentations-Cropped/test/\"\n",
    "\n",
    "# Cropped dataset (Black border removed )\n",
    "train_path = \"/home/mrb685/Datasets/Complete Random Cropped/\"\n",
    "val_path  = \"/home/mrb685/Datasets/Resized Cropped Dataset/val/\"\n",
    "test_path = \"/home/mrb685/Datasets/Resized Cropped Dataset/test/\"\n",
    "\n",
    "# train_path = \"/home/mrb685/Datasets/GAN Balanced/train/\"\n",
    "# val_path  = \"/home/mrb685/Datasets/GAN Balanced/test/\"\n",
    "\n",
    "CATEGORIES = [\"NV\", \"MEL\"]\n",
    "\n",
    "\n",
    "# # Retrain vanilla resnet50 with acc metrics\n",
    "# model_name = \"resnet_retrain_acc\"\n",
    "\n",
    "# Retrain no cw \n",
    "# model_name = \"resnet_retrain_acc_no_cw\"\n",
    "\n",
    "# Transfer learn with acc metrics\n",
    "# model_name = \"resnet_transfer_acc\"\n",
    "\n",
    "# shallow vs deep \n",
    "model_name = \"all_base_focal\"\n",
    "\n",
    "\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "# from keras import backend as K\n",
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "# session = tf.Session(config=config)\n",
    "# K.set_session(session)\n",
    "\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "\n",
    "# sess_cpu = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "# def get_available_gpus():\n",
    "#     local_device_protos = device_lib.list_local_devices()\n",
    "#     return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "# print(get_available_gpus())\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#sess_cpu = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "### CALLBACKS \n",
    "\n",
    "# tensorboard callback \n",
    "tensorboard_name = f\"Multimodal_skin_lesion_{model_name}\"\n",
    "tensorboard =  tf.keras.callbacks.TensorBoard(log_dir=f'logs/{tensorboard_name}')\n",
    "\n",
    "\n",
    "# model checkpoint callback \n",
    "checkpoint_path = f\"/home/mrb685/CheckPoints/{model_name}/cp.ckpt\" \n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "\n",
    "if not os.path.isfile(checkpoint_path):\n",
    "    open(checkpoint_path, 'w').close()\n",
    "        \n",
    "\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 monitor='val_accuracy',\n",
    "                                                mode='max')\n",
    "\n",
    "lr_on_plateau_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                                  patience=5, min_lr=0.00001)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor='val_accuracy', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as sklm\n",
    "\n",
    "\n",
    "class CustomMetricsAndEarlyStop(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "\n",
    "        \n",
    "        self.auc = []\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best_auc = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(f\"Log: {logs}\")\n",
    "        auc = logs[\"val_auc\"]\n",
    "        self.auc.append(auc)\n",
    "        current_auc = auc\n",
    "        \n",
    "        if np.greater(current_auc, self.best_auc):\n",
    "            save_path = f\"/home/mrb685/Saved Models/{model_name}_auc.h5\"\n",
    "            print(f\"val_auc improved from {self.best_auc} to {current_auc}. Saving model!\")\n",
    "            self.model.save(save_path)\n",
    "            self.best_auc = current_auc\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "                \n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print('Restoring model weights from the end of the best epoch.')\n",
    "                self.model.set_weights(self.best_weights)\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "class threadsafe_iter:\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self): # Py3\n",
    "        with self.lock:\n",
    "            return next(self.it)\n",
    "\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "@threadsafe_generator\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(train_generator)\n",
    "for x in range(10):\n",
    "    print(X[x].shape)\n",
    "    plt.imshow(X[x])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datagen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-35178116606a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_generator = datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datagen' is not defined"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        directory=train_path,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle = True,\n",
    "        classes=CATEGORIES,\n",
    "        class_mode=\"binary\")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    directory = val_path,\n",
    "    shuffle = True,\n",
    "    classes=CATEGORIES,\n",
    "    class_mode = \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K \n",
    "def focal_loss(alpha=0.25,gamma=2.0):\n",
    "    def focal_crossentropy(y_true, y_pred):\n",
    "        bce = K.binary_crossentropy(y_true, y_pred)\n",
    "        \n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1.- K.epsilon())\n",
    "        p_t = (y_true*y_pred) + ((1-y_true)*(1-y_pred))\n",
    "        \n",
    "        alpha_factor = 1\n",
    "        modulating_factor = 1\n",
    "\n",
    "        alpha_factor = y_true*alpha + ((1-alpha)*(1-y_true))\n",
    "        modulating_factor = K.pow((1-p_t), gamma)\n",
    "\n",
    "        # compute the final loss and return\n",
    "        return K.mean(alpha_factor*modulating_factor*bce, axis=-1)\n",
    "    return focal_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "ResNet50\n",
      "---------------------------------\n",
      "Datagen Param: 16, 224, 224\n",
      "Found 15372 images belonging to 2 classes.\n",
      "Found 3842 images belonging to 2 classes.\n",
      "{0: 0.6769420468557337, 1: 1.9128919860627178}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: old comet version (3.1.6) detected. current: 3.1.10 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/rsd96/skin-lesion-classification/e3d5b22f08214c84be3a9edb3da01f08\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15372\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Ignoring automatic log_parameter('verbose') because 'keras:verbose' is in COMET_LOGGING_PARAMETERS_IGNORE\n",
      "COMET INFO: Ignoring automatic log_parameter('do_validation') because 'keras:do_validation' is in COMET_LOGGING_PARAMETERS_IGNORE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 960 steps, validate for 240 steps\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Ignoring automatic log_metric('batch_batch') because 'keras:batch_batch' is in COMET_LOGGING_METRICS_IGNORE\n",
      "COMET INFO: Ignoring automatic log_metric('batch_size') because 'keras:batch_size' is in COMET_LOGGING_METRICS_IGNORE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "959/960 [============================>.] - ETA: 0s - loss: 0.0537 - auc: 0.8102Log: {'loss': 0.053736896793411155, 'auc': 0.81016254, 'val_loss': 0.08427276816219091, 'val_auc': 0.61471725, 'lr': 1e-05}\n",
      "val_auc improved from 0 to 0.6147172451019287. Saving model!\n",
      "960/960 [==============================] - 229s 238ms/step - loss: 0.0537 - auc: 0.8102 - val_loss: 0.0843 - val_auc: 0.6147\n",
      "Epoch 2/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0369 - auc: 0.8806Log: {'loss': 0.03689762870275313, 'auc': 0.880645, 'val_loss': 0.05638440641729782, 'val_auc': 0.71946925, 'lr': 1e-05}\n",
      "val_auc improved from 0.6147172451019287 to 0.7194692492485046. Saving model!\n",
      "960/960 [==============================] - 171s 178ms/step - loss: 0.0369 - auc: 0.8806 - val_loss: 0.0564 - val_auc: 0.7195\n",
      "Epoch 3/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0338 - auc: 0.9013Log: {'loss': 0.03381636344074939, 'auc': 0.90102935, 'val_loss': 0.05338323861748601, 'val_auc': 0.7439191, 'lr': 1e-05}\n",
      "val_auc improved from 0.7194692492485046 to 0.7439190745353699. Saving model!\n",
      "960/960 [==============================] - 173s 181ms/step - loss: 0.0338 - auc: 0.9010 - val_loss: 0.0534 - val_auc: 0.7439\n",
      "Epoch 4/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0321 - auc: 0.9116Log: {'loss': 0.03213929001743609, 'auc': 0.91162944, 'val_loss': 0.053321612180055426, 'val_auc': 0.73160094, 'lr': 1e-05}\n",
      "960/960 [==============================] - 169s 176ms/step - loss: 0.0321 - auc: 0.9116 - val_loss: 0.0533 - val_auc: 0.7316\n",
      "Epoch 5/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0308 - auc: 0.9192Log: {'loss': 0.03080551964202937, 'auc': 0.9193087, 'val_loss': 0.05459680968197063, 'val_auc': 0.7350027, 'lr': 1e-05}\n",
      "960/960 [==============================] - 170s 177ms/step - loss: 0.0308 - auc: 0.9193 - val_loss: 0.0546 - val_auc: 0.7350\n",
      "Epoch 6/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0295 - auc: 0.9262Log: {'loss': 0.029458854347952614, 'auc': 0.92621845, 'val_loss': 0.05488737929069127, 'val_auc': 0.75814956, 'lr': 1e-05}\n",
      "val_auc improved from 0.7439190745353699 to 0.7581495642662048. Saving model!\n",
      "960/960 [==============================] - 171s 178ms/step - loss: 0.0295 - auc: 0.9262 - val_loss: 0.0549 - val_auc: 0.7581\n",
      "Epoch 7/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0282 - auc: 0.9346Log: {'loss': 0.028228000188248795, 'auc': 0.9345985, 'val_loss': 0.055591459459780404, 'val_auc': 0.76150435, 'lr': 1e-05}\n",
      "val_auc improved from 0.7581495642662048 to 0.7615043520927429. Saving model!\n",
      "960/960 [==============================] - 171s 178ms/step - loss: 0.0282 - auc: 0.9346 - val_loss: 0.0556 - val_auc: 0.7615\n",
      "Epoch 8/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0270 - auc: 0.9393Log: {'loss': 0.02706032966203671, 'auc': 0.9392159, 'val_loss': 0.06155880614727115, 'val_auc': 0.7664243, 'lr': 1e-05}\n",
      "val_auc improved from 0.7615043520927429 to 0.766424298286438. Saving model!\n",
      "960/960 [==============================] - 172s 179ms/step - loss: 0.0271 - auc: 0.9392 - val_loss: 0.0616 - val_auc: 0.7664\n",
      "Epoch 9/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0252 - auc: 0.9483Log: {'loss': 0.025250853093422992, 'auc': 0.9482537, 'val_loss': 0.0578030102304183, 'val_auc': 0.7585276, 'lr': 1e-05}\n",
      "960/960 [==============================] - 170s 177ms/step - loss: 0.0252 - auc: 0.9483 - val_loss: 0.0578 - val_auc: 0.7585\n",
      "Epoch 10/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0237 - auc: 0.9542Log: {'loss': 0.023699927419340603, 'auc': 0.9542318, 'val_loss': 0.06635942616267129, 'val_auc': 0.7498569, 'lr': 1e-05}\n",
      "960/960 [==============================] - 170s 177ms/step - loss: 0.0237 - auc: 0.9542 - val_loss: 0.0664 - val_auc: 0.7499\n",
      "Epoch 11/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0232 - auc: 0.9551Log: {'loss': 0.02317525193436781, 'auc': 0.95513743, 'val_loss': 0.06484298358166901, 'val_auc': 0.7765611, 'lr': 1e-05}\n",
      "val_auc improved from 0.766424298286438 to 0.7765610814094543. Saving model!\n",
      "960/960 [==============================] - 171s 178ms/step - loss: 0.0232 - auc: 0.9551 - val_loss: 0.0648 - val_auc: 0.7766\n",
      "Epoch 12/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0224 - auc: 0.9589Log: {'loss': 0.02238874134733035, 'auc': 0.95896757, 'val_loss': 0.06751707127550617, 'val_auc': 0.76606923, 'lr': 1e-05}\n",
      "960/960 [==============================] - 169s 176ms/step - loss: 0.0224 - auc: 0.9590 - val_loss: 0.0675 - val_auc: 0.7661\n",
      "Epoch 13/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0211 - auc: 0.9638Log: {'loss': 0.02104721581197035, 'auc': 0.96385336, 'val_loss': 0.07006358939688653, 'val_auc': 0.76210827, 'lr': 1e-05}\n",
      "960/960 [==============================] - 170s 177ms/step - loss: 0.0210 - auc: 0.9639 - val_loss: 0.0701 - val_auc: 0.7621\n",
      "Epoch 14/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0205 - auc: 0.9667Log: {'loss': 0.020552052960273313, 'auc': 0.9666519, 'val_loss': 0.06558755853911861, 'val_auc': 0.7719596, 'lr': 1e-05}\n",
      "960/960 [==============================] - 169s 177ms/step - loss: 0.0205 - auc: 0.9667 - val_loss: 0.0656 - val_auc: 0.7720\n",
      "Epoch 15/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0191 - auc: 0.9718Log: {'loss': 0.019107081927722997, 'auc': 0.9718501, 'val_loss': 0.07246866794885136, 'val_auc': 0.7929838, 'lr': 1e-05}\n",
      "val_auc improved from 0.7765610814094543 to 0.7929837703704834. Saving model!\n",
      "960/960 [==============================] - 171s 178ms/step - loss: 0.0191 - auc: 0.9719 - val_loss: 0.0725 - val_auc: 0.7930\n",
      "Epoch 16/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0183 - auc: 0.9734Log: {'loss': 0.018242173737211445, 'auc': 0.9733885, 'val_loss': 0.07839480368226456, 'val_auc': 0.78039986, 'lr': 1e-05}\n",
      "960/960 [==============================] - 169s 176ms/step - loss: 0.0183 - auc: 0.9734 - val_loss: 0.0784 - val_auc: 0.7804\n",
      "Epoch 17/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0180 - auc: 0.9751Log: {'loss': 0.017967613099890586, 'auc': 0.9751348, 'val_loss': 0.07271817257084573, 'val_auc': 0.7717251, 'lr': 1e-05}\n",
      "960/960 [==============================] - 169s 176ms/step - loss: 0.0180 - auc: 0.9751 - val_loss: 0.0727 - val_auc: 0.7717\n",
      "Epoch 18/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0163 - auc: 0.9784Log: {'loss': 0.016350006287290143, 'auc': 0.97842705, 'val_loss': 0.08573812059087989, 'val_auc': 0.7679238, 'lr': 1e-05}\n",
      "960/960 [==============================] - 169s 176ms/step - loss: 0.0163 - auc: 0.9784 - val_loss: 0.0857 - val_auc: 0.7679\n",
      "Epoch 19/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0159 - auc: 0.9813Log: {'loss': 0.01591150721270173, 'auc': 0.9812971, 'val_loss': 0.07442970537910393, 'val_auc': 0.78386754, 'lr': 1e-05}\n",
      "960/960 [==============================] - 169s 176ms/step - loss: 0.0159 - auc: 0.9813 - val_loss: 0.0744 - val_auc: 0.7839\n",
      "Epoch 20/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0158 - auc: 0.9810Log: {'loss': 0.015810524722086723, 'auc': 0.9809858, 'val_loss': 0.09748444267703842, 'val_auc': 0.7618446, 'lr': 1e-05}\n",
      "960/960 [==============================] - 169s 176ms/step - loss: 0.0158 - auc: 0.9810 - val_loss: 0.0975 - val_auc: 0.7618\n",
      "Epoch 21/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0147 - auc: 0.9834Log: {'loss': 0.014660994700217044, 'auc': 0.98343843, 'val_loss': 0.09416338871621216, 'val_auc': 0.7763702, 'lr': 1e-05}\n",
      "960/960 [==============================] - 170s 177ms/step - loss: 0.0147 - auc: 0.9834 - val_loss: 0.0942 - val_auc: 0.7764\n",
      "Epoch 22/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0145 - auc: 0.9845Log: {'loss': 0.014453275966728274, 'auc': 0.9845343, 'val_loss': 0.10646116348313323, 'val_auc': 0.76406705, 'lr': 1e-05}\n",
      "960/960 [==============================] - 170s 177ms/step - loss: 0.0145 - auc: 0.9845 - val_loss: 0.1065 - val_auc: 0.7641\n",
      "Epoch 23/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0136 - auc: 0.9855Log: {'loss': 0.0135554112783893, 'auc': 0.98556584, 'val_loss': 0.1054382364161332, 'val_auc': 0.76916426, 'lr': 1e-05}\n",
      "960/960 [==============================] - 169s 176ms/step - loss: 0.0136 - auc: 0.9856 - val_loss: 0.1054 - val_auc: 0.7692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0132 - auc: 0.9871Log: {'loss': 0.013225003857515277, 'auc': 0.98707044, 'val_loss': 0.1068636734902005, 'val_auc': 0.7752646, 'lr': 1e-05}\n",
      "960/960 [==============================] - 169s 176ms/step - loss: 0.0132 - auc: 0.9871 - val_loss: 0.1069 - val_auc: 0.7753\n",
      "Epoch 25/100\n",
      "959/960 [============================>.] - ETA: 0s - loss: 0.0122 - auc: 0.9888Log: {'loss': 0.012213547159087977, 'auc': 0.9888385, 'val_loss': 0.12048146495265731, 'val_auc': 0.7698344, 'lr': 1e-05}\n",
      "Restoring model weights from the end of the best epoch.\n",
      "960/960 [==============================] - 169s 176ms/step - loss: 0.0122 - auc: 0.9888 - val_loss: 0.1205 - val_auc: 0.7698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/rsd96/skin-lesion-classification/e3d5b22f08214c84be3a9edb3da01f08\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     auc [25]                  : (0.8101625442504883, 0.9888384938240051)\n",
      "COMET INFO:     batch_auc [2400]          : (0.23076923191547394, 1.0)\n",
      "COMET INFO:     batch_loss [2400]         : (0.0001532227615825832, 0.9559524059295654)\n",
      "COMET INFO:     epoch_duration [25]       : (168.67961284331977, 228.57565608993173)\n",
      "COMET INFO:     loss [25]                 : (0.012213547159087977, 0.053736896793411155)\n",
      "COMET INFO:     lr [25]                   : 9.999999747378752e-06\n",
      "COMET INFO:     val_auc [25]              : (0.6147172451019287, 0.7929837703704834)\n",
      "COMET INFO:     val_loss [25]             : (0.053321612180055426, 0.12048146495265731)\n",
      "COMET INFO:     validate_batch_auc [600]  : (0.5652868151664734, 0.9818181395530701)\n",
      "COMET INFO:     validate_batch_loss [600] : (0.005319898948073387, 0.33035537600517273)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name             : base_randcrop_binary_cw_ResNet50\n",
      "COMET INFO:     trainable_params : 36433025\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Adam_amsgrad       : 1\n",
      "COMET INFO:     Adam_beta_1        : 0.9\n",
      "COMET INFO:     Adam_beta_2        : 0.999\n",
      "COMET INFO:     Adam_decay         : 1\n",
      "COMET INFO:     Adam_epsilon       : 1e-07\n",
      "COMET INFO:     Adam_learning_rate : 1e-05\n",
      "COMET INFO:     Adam_name          : Adam\n",
      "COMET INFO:     batch_size         : 1\n",
      "COMET INFO:     epochs             : 100\n",
      "COMET INFO:     samples            : 960\n",
      "COMET INFO:     steps              : 960\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     code                : 1 (19 KB)\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     model graph         : 1\n",
      "COMET INFO:     os packages         : 1\n",
      "COMET INFO: ---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00025: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 4320.304566383362\n"
     ]
    }
   ],
   "source": [
    "# CREATE MODEL \n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.applications import VGG19, ResNet50, ResNet101V2, ResNet152, DenseNet121, DenseNet169, DenseNet201\n",
    "# from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7 \n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# \"VGG19\", \"ResNet50\", \"ResNet152\", \"DenseNet121\", \"DenseNet201\", \"efficientnet_b0\", \n",
    "#               \"efficientnet_b1\", \"efficientnet_b2\", \"efficientnet_b3\", \"efficientnet_b4\"\n",
    "# \"efficientnet_b5\", \"efficientnet_b6\"\n",
    "\n",
    "# (16, 256, 224),  \n",
    "#                  (8, 256, 224), (8, 256, 240), (8, 300, 260), (8, 350, 300), (8, 400, 380)\n",
    "\n",
    "base_names = [\"ResNet50\"]\n",
    "datagen_param = [(16, 256, 224)]\n",
    "for i in range(len(base_names)):\n",
    "    \n",
    "    BATCH_SIZE, IMG_SIZE, IMG_CROP_SIZE = datagen_param[i]\n",
    "    \n",
    "    name = base_names[i]\n",
    "    if name == \"VGG19\":\n",
    "        base = VGG19(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"ResNet50\":\n",
    "        base = ResNet50(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"ResNet101V2\":\n",
    "        base = ResNet101V2(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"ResNet152\":\n",
    "        base = ResNet152(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"DenseNet121\":\n",
    "        base = DenseNet121(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"DenseNet169\":\n",
    "        base = DenseNet169(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"DenseNet201\":\n",
    "        base = DenseNet201(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"efficientnet_b0\":\n",
    "        base = EfficientNetB0(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"efficientnet_b1\":\n",
    "        base = EfficientNetB1(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"efficientnet_b2\":\n",
    "        base = EfficientNetB2(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"efficientnet_b3\":\n",
    "        base = EfficientNetB3(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"efficientnet_b4\":\n",
    "        base = EfficientNetB4(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"efficientnet_b5\":\n",
    "        base = EfficientNetB5(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"efficientnet_b6\":\n",
    "        base = EfficientNetB6(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    if name == \"efficientnet_b7\":\n",
    "        base = EfficientNetB7(input_shape=(IMG_CROP_SIZE,IMG_CROP_SIZE,3),weights=\"imagenet\",include_top=False)\n",
    "    \n",
    "    \n",
    "    model_name = f\"base_randcrop_binary_cw_{name}\"\n",
    "    print(\"---------------------------------\")\n",
    "    print(name)\n",
    "    print(\"---------------------------------\")\n",
    "    \n",
    "    np.random.seed(1234)\n",
    "    x = base.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, \"relu\")(x) \n",
    "    predictions = Dense(1, activation='sigmoid',name='output_layer')(x)\n",
    "    \n",
    "    model = Model(base.inputs, predictions)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(lr=1e-5)\n",
    "    model.compile(loss=focal_loss(), metrics=[tf.keras.metrics.AUC()],optimizer=opt)\n",
    "\n",
    "#     model.summary()\n",
    "#     nLayers = len(model.layers)\n",
    "# #     print(nLayers)\n",
    "    \n",
    "#     freeze_layers = int(nLayers * 0.20)\n",
    "    \n",
    "#     for l in range(freeze_layers):\n",
    "#         model.layers[l].trainable = False\n",
    "        \n",
    "        \n",
    "#     for layer in model.layers: \n",
    "#         print(layer.trainable)\n",
    "    \n",
    "    BATCH_SIZE, IMG_SIZE, IMG_CROP_SIZE = datagen_param[i]\n",
    "    IMG_SIZE = IMG_CROP_SIZE\n",
    "    \n",
    "    print(f\"Datagen Param: {BATCH_SIZE}, {IMG_SIZE}, {IMG_CROP_SIZE}\")\n",
    "    \n",
    "    datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                                 rotation_range=90,\n",
    "                                 horizontal_flip=True,\n",
    "                                 vertical_flip=True,\n",
    "                                 brightness_range=(0.8, 1.2),\n",
    "                                validation_split=0.2)\n",
    "\n",
    "    # datagen = ImageDataGenerator(rescale=1./255., validation_split=0.2)\n",
    "    # val_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        directory=train_path,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle = True,\n",
    "        classes=CATEGORIES,\n",
    "        subset=\"training\",\n",
    "        class_mode=\"binary\")\n",
    "\n",
    "    val_generator = datagen.flow_from_directory(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        directory = train_path,\n",
    "        shuffle = True,\n",
    "        subset=\"validation\",\n",
    "        classes=CATEGORIES,\n",
    "        class_mode = \"binary\")\n",
    "\n",
    "#     train_crop_generator = crop_generator(train_generator, IMG_CROP_SIZE)\n",
    "#     val_crop_generator = crop_generator(val_generator, IMG_CROP_SIZE)\n",
    "    \n",
    "    # Run model \n",
    "    \n",
    "    metrics = CustomMetricsAndEarlyStop(10)\n",
    "\n",
    "    # Add the following code anywhere in your machine learning file\n",
    "    # experiment = Experiment(api_key=\"ufQQBm4PhfVWxcwobrFapECw8\",\n",
    "    #                         project_name=\"skin-lesion-classification\", workspace=\"rsd96\")\n",
    "    import time\n",
    "    import multiprocessing\n",
    "    t=time.time()\n",
    "    \n",
    "\n",
    "\n",
    "    class_weights_list = class_weight.compute_class_weight(\n",
    "                   'balanced',\n",
    "                    np.unique(train_generator.classes), \n",
    "                    train_generator.classes)\n",
    "\n",
    "    class_weights = {}\n",
    "    for i in range(2):\n",
    "        class_weights[i] = class_weights_list[i]\n",
    "\n",
    "    print(class_weights)\n",
    "\n",
    "    \n",
    "    from comet_ml import Experiment\n",
    "    \n",
    "#     Add the following code anywhere in your machine learning file\n",
    "    experiment = Experiment(api_key=\"ufQQBm4PhfVWxcwobrFapECw8\",\n",
    "                        project_name=\"skin-lesion-classification\", workspace=\"rsd96\")\n",
    "    \n",
    "    experiment.set_name(model_name)\n",
    "    print(train_generator.samples)\n",
    "    hist = model.fit(train_generator,\n",
    "                               validation_data = val_generator,\n",
    "                               epochs=100,\n",
    "                               shuffle=True,\n",
    "                               workers = multiprocessing.cpu_count(),\n",
    "                               callbacks = [lr_on_plateau_callback, metrics],\n",
    "                               steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "                               validation_steps=val_generator.samples // BATCH_SIZE)\n",
    "\n",
    "    experiment.end()\n",
    "    print('Training time: %s' % (time.time() - t))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-713f14d678bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot and save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model sensitivity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "# Plot and save model \n",
    "\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('model sensitivity')\n",
    "plt.ylabel('sensitivity')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# model.save(f\"{model_name}.model\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data generator \n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "datagen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    directory=test_path,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle = True,\n",
    "    classes=CATEGORIES,\n",
    "    class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "ResNet50\n",
      "---------------------------------\n",
      "Datagen Param: 16, 224, 224\n",
      "Found 15372 images belonging to 2 classes.\n",
      "Found 3842 images belonging to 2 classes.\n",
      "241/241 [==============================] - 59s 243ms/step\n",
      "[5.5342491e-05 2.5259197e-04 3.1603925e-04 ... 9.9833244e-01 9.9947137e-01\n",
      " 9.9989438e-01]\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mrb685/Confusions/ResNet50.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-624a1d78fd41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/home/mrb685/Confusions/{name}.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Classification Report'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCATEGORIES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mrb_gpu/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mrb_gpu/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mrb_gpu/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2092\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mrb_gpu/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                     \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[1;32m    532\u001b[0m                                self.figure.dpi, metadata=metadata)\n",
      "\u001b[0;32m~/anaconda3/envs/mrb_gpu/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mrb_gpu/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mrb_gpu/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mrb685/Confusions/ResNet50.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"VGG19\", \"ResNet50\", \"ResNet101V2\", \"ResNet152\", \"DenseNet121\", \"DenseNet201\",\n",
    "#                \"efficientnet_b0\", \"efficientnet_b1\", \"efficientnet_b2\", \"efficientnet_b3\", \"efficientnet_b4\",\n",
    "#                \"efficientnet_b5\", \"efficientnet_b6\",\"efficientnet_b7\"\n",
    "\n",
    "model_names = [\"ResNet50\"]\n",
    "datagen_params = [ (16, 256, 224)]\n",
    "# model_names = [\"efficientnet_b4\"]\n",
    "for i in range(len(model_names)): \n",
    "    name = model_names[i]\n",
    "    print(\"---------------------------------\")\n",
    "    print(name)\n",
    "    print(\"---------------------------------\")\n",
    "    \n",
    "#     layers = [\"25\", \"50\", \"75\"]\n",
    "#     for layer in layers:\n",
    "    model_name = f\"base_randcrop_binary_cw_{name}_auc\" \n",
    "    model.load_weights(f\"/home/mrb685/Saved Models/{model_name}.h5\")\n",
    "\n",
    "\n",
    "    BATCH_SIZE, IMG_SIZE, IMG_CROP_SIZE = datagen_params[i]\n",
    "    IMG_SIZE = IMG_CROP_SIZE\n",
    "    \n",
    "    print(f\"Datagen Param: {BATCH_SIZE}, {IMG_SIZE}, {IMG_CROP_SIZE}\")\n",
    "    \n",
    "    datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                                 rotation_range=90,\n",
    "                                 horizontal_flip=True,\n",
    "                                 vertical_flip=True,\n",
    "                                 brightness_range=(0.8, 1.2),\n",
    "                                validation_split=0.2)\n",
    "\n",
    "    # datagen = ImageDataGenerator(rescale=1./255., validation_split=0.2)\n",
    "    # val_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        target_size=(IMG_CROP_SIZE, IMG_CROP_SIZE),\n",
    "        directory=train_path,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle = True,\n",
    "        classes=CATEGORIES,\n",
    "        subset=\"training\",\n",
    "        class_mode=\"binary\")\n",
    "    \n",
    "    val_generator = datagen.flow_from_directory(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        target_size=(IMG_CROP_SIZE, IMG_CROP_SIZE),\n",
    "        directory = train_path,\n",
    "        shuffle = True,\n",
    "        subset=\"validation\",\n",
    "        classes=CATEGORIES,\n",
    "        class_mode = \"binary\")\n",
    "\n",
    "    \n",
    "\n",
    "    #Confution Matrix and Classification Report\n",
    "    Y_pred = model.predict(val_generator, verbose=1 )\n",
    "    print(np.unique(Y_pred))\n",
    "    y_pred = Y_pred < 0.2\n",
    "    \n",
    "\n",
    "    print('Confusion Matrix')\n",
    "    cm = confusion_matrix(val_generator.classes, y_pred, normalize=\"true\")\n",
    "\n",
    "    ax= plt.subplot()\n",
    "\n",
    "    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted');\n",
    "    ax.set_ylabel('True'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(CATEGORIES); \n",
    "    ax.yaxis.set_ticklabels(CATEGORIES);\n",
    "    plt.show()\n",
    "\n",
    "    plt.savefig(f'/home/mrb685/Confusions/{name}.png')\n",
    "    print('Classification Report')\n",
    "    report = classification_report(val_generator.classes, y_pred, target_names=CATEGORIES,labels=range(2), output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    print(classification_report(val_generator.classes, y_pred, target_names=CATEGORIES))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(val_generator.classes, y_pred, pos_label=1)\n",
    "    print(f\"AUC : {auc(fpr, tpr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label=f'Keras (area = {auc}')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"VGG19\", \"ResNet50\", \"ResNet101V2\", \"ResNet152\", \"DenseNet121\", \"DenseNet201\",\n",
    "#                \"efficientnet_b0\", \"efficientnet_b1\", \"efficientnet_b2\", \"efficientnet_b3\", \"efficientnet_b4\",\n",
    "#                \"efficientnet_b5\", \"efficientnet_b6\",\"efficientnet_b7\"\n",
    "\n",
    "model_names = [\"ResNet50\", \"ResNet152\", \"DenseNet121\", \"DenseNet201\",\n",
    "               \"efficientnet_b0\", \"efficientnet_b1\", \"efficientnet_b2\", \"efficientnet_b3\", \"efficientnet_b4\"]\n",
    "\n",
    "# model_names = [\"efficientnet_b4\"]\n",
    "for name in model_names: \n",
    "    print(\"---------------------------------\")\n",
    "    print(name)\n",
    "    print(\"---------------------------------\")\n",
    "    \n",
    "#     layers = [\"25\", \"50\", \"75\"]\n",
    "#     for layer in layers:\n",
    "    model_name = f\"base_isic_randcrop_{name}\" \n",
    "    model = load_model(f\"/home/mrb685/Saved Models/{model_name}.h5\")\n",
    "\n",
    "\n",
    "    num_of_test_samples = 1651\n",
    "\n",
    "    #Confution Matrix and Classification Report\n",
    "    Y_pred = model.predict_generator(val_generator, num_of_test_samples // BATCH_SIZE +1, verbose=1)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    target_names = [\"NV\", \"MEL\", \"BCC\", \"BKL\", \"AK\", \"SCC\", \"VASC\", \"DF\"]\n",
    "\n",
    "    print('Confusion Matrix')\n",
    "    cm = confusion_matrix(val_generator.classes, y_pred, normalize='true')\n",
    "\n",
    "    ax= plt.subplot()\n",
    "\n",
    "    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted');\n",
    "    ax.set_ylabel('True'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(target_names); \n",
    "    ax.yaxis.set_ticklabels(target_names);\n",
    "    plt.show()\n",
    "\n",
    "    plt.savefig(f'/home/mrb685/Confusions/{name}.png')\n",
    "    print('Classification Report')\n",
    "    report = classification_report(val_generator.classes, y_pred, target_names=target_names, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    print(classification_report(val_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(val_generator.classes, y_pred, pos_label=2)\n",
    "    print(f\"AUC : {auc(fpr, tpr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"VGG19\", \"ResNet50\", \"ResNet101V2\", \"ResNet152\", \"DenseNet121\", \"DenseNet201\",\n",
    "               \"efficientnet_b0\", \"efficientnet_b1\", \"efficientnet_b2\", \"efficientnet_b3\", \"efficientnet_b4\",\n",
    "               \"efficientnet_b5\", \"efficientnet_b6\",\"efficientnet_b7\"]\n",
    "\n",
    "for name in model_names:\n",
    "    print(\"---------------------------------\")\n",
    "    print(name)\n",
    "    print(\"---------------------------------\")\n",
    "    model_name = f\"base_nocw_{name}\" \n",
    "    model = load_model(f\"/home/mrb685/Saved Models/{model_name}.h5\")\n",
    "\n",
    "    pred = model.evaluate(val_generator)\n",
    "\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "size = 600, 600\n",
    "count = 0\n",
    "save_path = \"/home/mrb685/Datasets/ISIC Resized/\"\n",
    "path = \"/home/mrb685/Datasets/ISIC Only/\"\n",
    "for root, dirs, files in os.walk(save_path):\n",
    "#     for file in files:\n",
    "#         c_name = root.split(\"/\")[-1]\n",
    "#         if c_name not in [\"VASC\", \"MEL\", \"AK\", \"BKL\", \"NV\"]:\n",
    "#             save_dir = os.path.join(save_path, c_name, file)\n",
    "#             print(save_dir)\n",
    "#             im = Image.open(root + \"/\" + file)\n",
    "#             print(im.width, im.height)\n",
    "#             im.thumbnail(size, Image.ANTIALIAS)\n",
    "#             print(im.width, im.height)\n",
    "#             im.save(save_dir, \"JPEG\")\n",
    "    print(root)\n",
    "    print(len(files))\n",
    "    count = count + len(files)\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_kernel",
   "language": "python",
   "name": "mrb_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
